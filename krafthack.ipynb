{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "krafthack.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smedegaard/krafthack/blob/main/krafthack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Krafthack 2022\n",
        "Authors and developers: Anders Pedersen and HÃ¥kon Holte"
      ],
      "metadata": {
        "id": "Gjcih1WjAjR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GlO467wVsiy",
        "outputId": "b9cc2fe1-acc4-46e7-cd5b-58b0aa6ba430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "Nbgy3Ptrd8-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages and set some global parameters\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 10)"
      ],
      "metadata": {
        "id": "T8M_zQsqdR8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and view dataset - needs path to dataset\n",
        "dataset_df = pd.read_parquet('')\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "ZMR5mpQhdZG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get relevant facts about dataset\n",
        "n_rows = len(dataset_df)\n",
        "n_cols = len(dataset_df.columns)\n",
        "\n",
        "# Get count, mean and other relevant statistics\n",
        "dataset_description = dataset_df.describe()\n",
        "\n",
        "print(\"Dataset consists of:\\n\", n_rows, \"rows\\n\", n_cols, \"columns\\n\")\n",
        "print(\"Description of dataset:\\n\", dataset_description)"
      ],
      "metadata": {
        "id": "bd7m8WmTdivY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlation plot to consider correlations between variables\n",
        "dataset_corr = dataset_df.corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 15))\n",
        "sns.heatmap(dataset_corr, annot=True, cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)"
      ],
      "metadata": {
        "id": "7Edbb9iedm4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change x_var and y_var to desired variables to make scatterplots grouped by mode (start or operational)\n",
        "x_var = \"Bolt_1_Tensile\"\n",
        "y_var = \"Turbine_Guide Vane Opening\"\n",
        "by_var = \"mode\"\n",
        "groups = dataset_df.groupby(by_var)\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "for name, group in groups:\n",
        "  plt.plot(group[x_var], group[y_var], marker='o', linestyle='', markersize=1, label=name)\n",
        "ax.set_xlabel(x_var)\n",
        "ax.set_ylabel(y_var)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "PyCrty5Ndo2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "4WnAQduKeXKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import warnings as wgs"
      ],
      "metadata": {
        "id": "LBpbrBrN9Fvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables not to be included as features\n",
        "drop_vars = ['Bolt_1_Tensile', 'Bolt_2_Tensile', 'Bolt_3_Tensile', 'Bolt_4_Tensile', 'Bolt_5_Tensile', 'Bolt_6_Tensile', \n",
        "             'Bolt_1_Torsion', 'Bolt_2_Torsion', 'Bolt_3_Torsion', 'Bolt_4_Torsion', 'Bolt_5_Torsion', 'Bolt_6_Torsion', \n",
        "             'Bolt_1_Steel tmp', 'mode']\n",
        "\n",
        "# Create mode indicator variable\n",
        "dataset_df['is_mode_start'] = (dataset_df['mode'] == 'start') * 1"
      ],
      "metadata": {
        "id": "3HyJInKhd55S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove null values. For this, vibration variables are removed from the dataset, as these have many occurences of null values\n",
        "X_clean = dataset_df.drop(['lower_bearing_vib_vrt',\t'turbine_bearing_vib_vrt'], axis=1).dropna()"
      ],
      "metadata": {
        "id": "KfnzZdxRedIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data split function to split data into equally sized chunks\n",
        "def split_dataframe(df, chunk_size = 10000): \n",
        "    chunks = list()\n",
        "    num_chunks = math.ceil(len(df) / chunk_size)\n",
        "    for i in range(num_chunks):\n",
        "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "rZok9zVqedos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use split_dataframe function to split datafram into chunks with chunk_size elements in each chunk\n",
        "chunk_size = 10000\n",
        "df_chunks = pd.Series(split_dataframe(X_clean, chunk_size))"
      ],
      "metadata": {
        "id": "ZMGM41CQemgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set size of training data set and shuffle chunk indices\n",
        "train_size = 0.8\n",
        "shuffled_idxs = np.arange(0, len(df_chunks), 1)\n",
        "np.random.shuffle(shuffled_idxs)\n",
        "\n",
        "# Set chunks to be placed in train, validation and test set\n",
        "train_chunks_temp = shuffled_idxs[:math.ceil(train_size*len(shuffled_idxs))]\n",
        "test_chunks = shuffled_idxs[math.ceil(train_size*len(shuffled_idxs)):]\n",
        "train_chunks = train_chunks_temp[:math.ceil((2-1/train_size)*len(train_chunks_temp))]\n",
        "val_chunks = train_chunks_temp[math.ceil((2-1/train_size)*len(train_chunks_temp)):]\n",
        "\n",
        "# Set training, validation and test data\n",
        "train_data = pd.concat(list(df_chunks[train_chunks]))\n",
        "val_data = pd.concat(list(df_chunks[val_chunks]))\n",
        "test_data = pd.concat(list(df_chunks[test_chunks]))\n",
        "\n",
        "# Set up train, validation and test feature dataframes\n",
        "X_train = train_data.drop(drop_vars, inplace=False, axis=1)\n",
        "X_val = val_data.drop(drop_vars, inplace=False, axis=1)\n",
        "X_test = test_data.drop(drop_vars, inplace=False, axis=1)\n",
        "\n",
        "# Set up train, validation and test target dataframes\n",
        "y_train_df = train_data[['Bolt_1_Tensile', 'Bolt_2_Tensile', 'Bolt_3_Tensile', 'Bolt_4_Tensile', 'Bolt_5_Tensile', 'Bolt_6_Tensile']]\n",
        "y_val_df = val_data[['Bolt_1_Tensile', 'Bolt_2_Tensile', 'Bolt_3_Tensile', 'Bolt_4_Tensile', 'Bolt_5_Tensile', 'Bolt_6_Tensile']]\n",
        "y_test_df = test_data[['Bolt_1_Tensile', 'Bolt_2_Tensile', 'Bolt_3_Tensile', 'Bolt_4_Tensile', 'Bolt_5_Tensile', 'Bolt_6_Tensile']]"
      ],
      "metadata": {
        "id": "pkJfOq82exl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling features for using chunk split\n",
        "with wgs.catch_warnings():\n",
        "  wgs.simplefilter(\"ignore\")\n",
        "  scaler1 = MinMaxScaler()\n",
        "  scaler2 = MinMaxScaler((-1, 1))\n",
        "  keep_vars = dataset_df.columns.drop([*drop_vars, 'Unit_4_Reactive Power', 'is_mode_start', 'lower_bearing_vib_vrt', 'turbine_bearing_vib_vrt'])\n",
        "  for keep_var in keep_vars:\n",
        "    scaler1.fit(X_train[[keep_var]])\n",
        "    X_train[[keep_var]] = scaler1.transform(X_train[[keep_var]])\n",
        "    X_val[[keep_var]] = scaler1.transform(X_val[[keep_var]])\n",
        "    X_test[[keep_var]] = scaler1.transform(X_test[[keep_var]])\n",
        "  scaler2.fit(X_train[['Unit_4_Reactive Power']])\n",
        "  X_train[['Unit_4_Reactive Power']] = scaler2.transform(X_train[['Unit_4_Reactive Power']])\n",
        "  X_val[['Unit_4_Reactive Power']] = scaler2.transform(X_val[['Unit_4_Reactive Power']])\n",
        "  X_test[['Unit_4_Reactive Power']] = scaler2.transform(X_test[['Unit_4_Reactive Power']])\n",
        "\n",
        "# Verify that values are scaled\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "WTHKyD9pfymC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling targets. As above, the test targets are scaled using the same scaler as the train targets. A seperate scaler is fitted for each of the six bolt tensil columns\n",
        "# Scaling targets for using chunk split\n",
        "with wgs.catch_warnings():\n",
        "  wgs.simplefilter(\"ignore\")\n",
        "  y_scalers = [MinMaxScaler() for i in range(0, 6)]\n",
        "  for i in range(0, 6):\n",
        "    col_i = y_train_df[y_train_df.columns[i]]\n",
        "    y_scalers[i].fit(col_i.values.reshape(-1, 1))\n",
        "    y_train_df[y_train_df.columns[i]] = y_scalers[i].transform(col_i.values.reshape(-1, 1))\n",
        "    y_val_df[y_val_df.columns[i]] = y_scalers[i].transform(y_val_df[y_val_df.columns[i]].values.reshape(-1, 1))\n",
        "    y_test_df[y_test_df.columns[i]] = y_scalers[i].transform(y_test_df[y_test_df.columns[i]].values.reshape(-1, 1))\n",
        "\n",
        "# Verify that values are scaled\n",
        "y_train_df.head()"
      ],
      "metadata": {
        "id": "bPYcPPSHgGVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "a8cpR04HgxsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras.regularizers import l2, l1"
      ],
      "metadata": {
        "id": "JElowRbygt8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors for chunk data\n",
        "X_train_chunks = pd.Series(split_dataframe(X_train, chunk_size))\n",
        "X_train_tensors = [tf.convert_to_tensor(chunk) for chunk in X_train_chunks]\n",
        "\n",
        "y_train_chunks = pd.Series(split_dataframe(y_train_df, chunk_size))\n",
        "y_train_tensors = [tf.convert_to_tensor(chunk) for chunk in y_train_chunks]\n",
        "\n",
        "X_val_tensor = tf.convert_to_tensor(X_val)\n",
        "y_val_tensor = tf.convert_to_tensor(y_val_df[y_val_df.columns[0]])"
      ],
      "metadata": {
        "id": "CHhrkACbg33w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some hyperparameters\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32 #16\n",
        "LEARNING_RATE = 0.01\n",
        "REGULARIZATION_PARAM = 0.02\n",
        "LOSS_FUNC = tf.keras.losses.MeanSquaredError()\n",
        "METRIC_NAME = 'MSE'\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# Define fully connected network\n",
        "model1 = tf.keras.models.Sequential()\n",
        "model1.add(tf.keras.Input(shape=(7,)))\n",
        "model1.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=l2(REGULARIZATION_PARAM), bias_regularizer=l1(REGULARIZATION_PARAM),\n",
        "                                  kernel_initializer=tf.keras.initializers.GlorotNormal(), bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0.1, stddev=0.1)))\n",
        "model1.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model1.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=l2(REGULARIZATION_PARAM), bias_regularizer=l1(REGULARIZATION_PARAM),\n",
        "                                  kernel_initializer=tf.keras.initializers.GlorotNormal(), bias_initializer=tf.keras.initializers.TruncatedNormal(mean=0.1, stddev=0.1)))\n",
        "model1.add(tf.keras.layers.Dense(1, activation='relu'))\n",
        "model1.output_shape"
      ],
      "metadata": {
        "id": "VvmTkNIpndOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure early stopping\n",
        "MIN_DELTA = 0\n",
        "PATIENCE = 5\n",
        "\n",
        "# Helper function for checking validation loss and early stopping\n",
        "def check_loss_and_early_stopping(val_loss_history, best_val_loss, es_counter, best_model_idx, model_idx):\n",
        "    if (len(val_loss_history) == 1):\n",
        "      best_val_loss = val_loss_history[0]\n",
        "    elif ((val_loss_history[-1] - best_val_loss) >= -MIN_DELTA):\n",
        "      es_counter += 1\n",
        "    else:\n",
        "      es_counter = 0\n",
        "      best_val_loss = val_loss_history[-1]\n",
        "      best_model_idx = model_idx\n",
        "    return best_val_loss, es_counter, best_model_idx"
      ],
      "metadata": {
        "id": "RTmw7quAnvUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set metrics for monitoring validation loss\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "loss = mean_squared_error\n",
        "if METRIC_NAME == 'MAPE':\n",
        "  loss = mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "RGGYsVUPnyzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model and view summary\n",
        "model1.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),\n",
        "              loss=LOSS_FUNC,\n",
        "              metrics=['mse', 'mape'])\n",
        "\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "EBeME5Qjn3ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train fully connected model for bolt 1 with chunks\n",
        "history = 0\n",
        "es_counter = 0\n",
        "best_val_loss = 100\n",
        "best_model_idx = 0\n",
        "val_loss_history = []\n",
        "model_history = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  if es_counter > PATIENCE:\n",
        "    print(\"Patience reached, stopping early at epoch #{}\".format(epoch))\n",
        "    break\n",
        "  print('epoch #{}'.format(epoch))\n",
        "  for i in range(0, len(X_train_tensors)):\n",
        "    model_history.append(model1.fit(x=X_train_tensors[i], y=y_train_tensors[i][:, 0], batch_size=BATCH_SIZE))\n",
        "    if (i%7 == 0 and i != 0):\n",
        "      print(\"Scoring validation set...\")\n",
        "      val_preds = model1.predict(X_val_tensor)\n",
        "      print(\"Calculating loss...\")\n",
        "      val_loss_history.append(loss(y_val_tensor, val_preds))\n",
        "      print(\"Checking loss and early stopping...\")\n",
        "      best_val_loss, es_counter, best_model_idx = check_loss_and_early_stopping(val_loss_history, best_val_loss, es_counter, best_model_idx, \n",
        "                                                                                i+epoch*len(X_train_tensors))\n",
        "      print(\"Current validation loss:\", val_loss_history[-1])\n",
        "      print(\"Best validation loss:\", best_val_loss)\n",
        "      print(\"Early stopping counter:\", es_counter)\n",
        "      if es_counter > PATIENCE:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "bkDwKbhHn6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate and save Model"
      ],
      "metadata": {
        "id": "rGgpYEDZ3SpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model\n",
        "best_model = model_history[best_model_idx].model"
      ],
      "metadata": {
        "id": "lw3edLk93jiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score model on test set\n",
        "X_test_tensor = tf.convert_to_tensor(X_test)\n",
        "y_test_tensor = tf.convert_to_tensor(y_test_df[y_test_df.columns[0]])\n",
        "\n",
        "y_pred = best_model.predict(X_test_tensor)\n",
        "\n",
        "print('mse:', mean_squared_error(y_test_tensor, y_pred), 'mape:', mean_absolute_percentage_error(y_test_tensor, y_pred))"
      ],
      "metadata": {
        "id": "cgmQeK743pvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to Google Drive\n",
        "model_name = \"\"\n",
        "savepath = \"/content/gdrive/My Drive/Datasets/Krafthack/Models/\" + model_name\n",
        "tf.keras.models.save_model(best_model, filepath=savepath)"
      ],
      "metadata": {
        "id": "Tvkf_43c3qki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}